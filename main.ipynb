{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training, dev and unlabeled test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following provides a starting code (Python 3) of how to read the labeled training and dev cipher text, and unlabeled test cipher text, into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16220\n",
      "[[0, 'lkêcê yoúc cêêö y#êjl lw mówám Újám j Úêê# ütlk Úol lkêú z#ê ctöé8ú ówl xoóóú éê#xw#öê#c .'], [0, '6êcétlê jolêot8 zc éê#xw#öjóáê , tl zc j #jlkê# 8tcl8êcc jöÚ8ê 6wüó lkê öt668ê wx lkê #wj6 , ükê#ê lkê lkêöjltá t#wótêc j#ê lww wÚ2twoc jó6 lkê cê+oj8 éw8tltác lww cöoy .'], [0, 'tx lktc kw8t6jú öw2tê tc coééwcê6 lw Úê j ytxl , cwöêÚw6ú oóü#jééê6 tl êj#8ú , lwwm wol j88 lkê yww6 cloxx , jó6 8êxl Úêktó6 lkê á#jé ( 8tlê#j88ú ) .']]\n"
     ]
    }
   ],
   "source": [
    "for x in open('./train_enc.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r').split('\\t')\n",
    "    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
    "    x[0] = int(x[0]) \n",
    "    train.append(x)\n",
    "print(len(train))\n",
    "print(train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027\n",
      "[[1, 'ów8jó Ú#j2ê8ú l#êj6c ükê#ê xêü jöê#tájó xt8öc 6j#ê lw 6ê82ê 77 tólw lkê üw#86 wx jöÚt2j8êóáê jó6 jöÚtyotlú <<<'], [0, 'ê2êó öo#ékú zc ê+éê#l áwötá ltötóy jó6 xjöê6 ákj#tcöj áj ózl #êcáoê lktc êxxw#l .'], [1, 'üt88 jcco#ê68ú #jóm jc wóê wx lkê á8ê2ê#êcl , öwcl 6êáêélt2ê8ú jöoctóy áwöê6têc wx lkê úêj# .']]\n"
     ]
    }
   ],
   "source": [
    "for x in open('./dev_enc.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r').split('\\t')\n",
    "    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
    "    x[0] = int(x[0]) \n",
    "    dev.append(x)\n",
    "print (len(dev))\n",
    "print (dev[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different from 'train' and 'dev' that are both list of tuples, 'test' will be just a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2028\n",
      "['j 6t6jáltá jó6 6o88 6wáoöêólj#ú y8w#txútóy cwxlüj#ê jój#ákú .', 'ówlktóy cltámc , #êj88ú , ê+áêél j 8tóyê#tóy á#êêétóêcc wóê xêê8c x#wö Úêtóy 6#jyyê6 lk#woyk j cj6 , cw#6t6 oót2ê#cê wx yoóc , 6#oyc , j2j#táê jó6 6jöjyê6 6#êjöc .', 'öo#ékú jó6 üt8cwó jáloj88ú öjmê j é#êllú yww6 lêjö <<< Úol lkê é#wvêál co##woó6tóy lkêö tc 6tcl#êcctóy8ú #wlê .']\n"
     ]
    }
   ],
   "source": [
    "for x in open('./test_enc_unlabeled.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r')\n",
    "    test.append(x)\n",
    "print (len(test))\n",
    "print (test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can split every sentence into lists of words by white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = [[x[0], x[1].split(' ')] for x in train]\n",
    "dev_split = [[x[0], x[1].split(' ')] for x in dev]\n",
    "test_split = [[x.split(' ')] for x in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may choose to experiment with different methods using your program. However, you need to embed the training and inference processes at here. We will use your prediction on the unlabeled test data to grade, while checking this part to understand how your method has produced the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a Look at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_enc.tsv', sep='\\t', header = None, names = ['label', 'text'], encoding='utf-8')\n",
    "test_df = pd.read_csv('dev_enc.tsv', sep='\\t', header = None, names = ['label', 'text'], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lkêcê yoúc cêêö y#êjl lw mówám Újám j Úêê# ütl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6êcétlê jolêot8 zc éê#xw#öjóáê , tl zc j #jlkê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tx lktc kw8t6jú öw2tê tc coééwcê6 lw Úê j ytxl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>vocl ükêó úwo lktóm lkjl ê2ê#ú éwcctÚ8ê jóy8ê ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>yt2ê á#ê6tl lw ê2ê#úwóê x#wö #wÚtócwó 6wüó lw ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  lkêcê yoúc cêêö y#êjl lw mówám Újám j Úêê# ütl...\n",
       "1      0  6êcétlê jolêot8 zc éê#xw#öjóáê , tl zc j #jlkê...\n",
       "2      0  tx lktc kw8t6jú öw2tê tc coééwcê6 lw Úê j ytxl...\n",
       "3      1  vocl ükêó úwo lktóm lkjl ê2ê#ú éwcctÚ8ê jóy8ê ...\n",
       "4      1  yt2ê á#ê6tl lw ê2ê#úwóê x#wö #wÚtócwó 6wüó lw ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ów8jó Ú#j2ê8ú l#êj6c ükê#ê xêü jöê#tájó xt8öc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ê2êó öo#ékú zc ê+éê#l áwötá ltötóy jó6 xjöê6 á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>üt88 jcco#ê68ú #jóm jc wóê wx lkê á8ê2ê#êcl , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>tl kjc j ájxxêtójlê6 , c8wééú Ú#t88tjóáê , céj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>lww öoák wx clw#úlê88tóy öw2êc jüjú x#wö cw8wó...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  ów8jó Ú#j2ê8ú l#êj6c ükê#ê xêü jöê#tájó xt8öc ...\n",
       "1      0  ê2êó öo#ékú zc ê+éê#l áwötá ltötóy jó6 xjöê6 á...\n",
       "2      1  üt88 jcco#ê68ú #jóm jc wóê wx lkê á8ê2ê#êcl , ...\n",
       "3      1  tl kjc j ájxxêtójlê6 , c8wééú Ú#t88tjóáê , céj...\n",
       "4      1  lww öoák wx clw#úlê88tóy öw2êc jüjú x#wö cw8wó..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'Doe', 'Was', 'Here', 'Lol', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_cipher(stringerino):\n",
    "  stringerino = stringerino.rstrip('\\n\\r').split()\n",
    "  tokens = [token for token in stringerino if token not in [',','.']]\n",
    "  return tokens\n",
    "tokenize_cipher('John Doe Was Here Lol , ! .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_or_test(df, top_words, maxlen, fdist = None, train = True):\n",
    "  # top_words = 20000\n",
    "  # maxlen = 200 \n",
    "  words_list = []\n",
    "  for entry in df['text']:\n",
    "    splitted = entry.split()\n",
    "    for word in splitted:\n",
    "      if word not in ['.', ',']:\n",
    "        words_list.append(word)\n",
    "  \n",
    "  if not fdist:\n",
    "    fdist = FreqDist(words_list)\n",
    "\n",
    "  \n",
    "  train_list = []\n",
    "  train_arr = df.text.to_numpy()\n",
    "\n",
    "  for text in train_arr:\n",
    "    text = text.split()\n",
    "    text = [word for word in text]\n",
    "    text = \" \".join(text)\n",
    "    train_list.append(text)\n",
    "\n",
    "  train_arr = np.asarray(train_list)\n",
    "  df['text'] = train_arr\n",
    "\n",
    "  #tokenizing\n",
    "  df['text'] = df.text.apply(tokenize_cipher)\n",
    "\n",
    "  # converting text tokens to freq dist ranks\n",
    "\n",
    "\n",
    "  terms = [term for term, count in fdist.most_common(n=top_words)]\n",
    "  df.text = df.text.apply(lambda text:\n",
    "                                    [terms.index(term) if term in terms else 0 \n",
    "                                    for term in text])\n",
    "\n",
    "  x = df.text\n",
    "  y = df.label\n",
    "  \n",
    "  x = sequence.pad_sequences(x, maxlen=maxlen)\n",
    "\n",
    "  if train:\n",
    "    return x, y, fdist\n",
    "  else:\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_predict_data(df, top_words, maxlen, fdist = None):\n",
    "  # top_words = 20000\n",
    "  # maxlen = 200 \n",
    "  words_list = []\n",
    "  for entry in df['text']:\n",
    "    splitted = entry.split()\n",
    "    for word in splitted:\n",
    "      words_list.append(word)\n",
    "  \n",
    "  if not fdist:\n",
    "    fdist = FreqDist(words_list)\n",
    "\n",
    "  train_list = []\n",
    "  train_arr = df.text.to_numpy()\n",
    "\n",
    "  for text in train_arr:\n",
    "    text = text.split()\n",
    "    text = [word for word in text]\n",
    "    text = \" \".join(text)\n",
    "    train_list.append(text)\n",
    "\n",
    "  train_arr = np.asarray(train_list)\n",
    "  df['text'] = train_arr\n",
    "\n",
    "  #tokenizing\n",
    "  df['text'] = df.text.apply(tokenize_cipher)\n",
    "\n",
    "  # converting text tokens to freq dist ranks\n",
    "\n",
    "\n",
    "  terms = [term for term, count in fdist.most_common(n=top_words)]\n",
    "  df.text = df.text.apply(lambda text:\n",
    "                                    [terms.index(term) if term in terms else 0 \n",
    "                                    for term in text])\n",
    "\n",
    "  x = df.text\n",
    "\n",
    "  x = sequence.pad_sequences(x, maxlen=maxlen)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Transformer Block (Stack of Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.feed_forward = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "\n",
    "        attention_output = self.attention(inputs, inputs)\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        out1 = self.normalize1(inputs + attention_output)\n",
    "        feed_forward_output = self.feed_forward(out1)\n",
    "        feed_forward_output = self.dropout2(feed_forward_output, training=training)\n",
    "        out2 = self.normalize2(out1 + feed_forward_output)\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Embedding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  maxlen = 200\n",
    "  vocab_size = 20000\n",
    "  embed_dim = 64  # Embedding size for each token\n",
    "  num_heads = 2  # Number of attention heads\n",
    "  ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "  inputs = layers.Input(shape=(maxlen,))\n",
    "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "  x = embedding_layer(inputs)\n",
    "  \n",
    "  transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "  x = transformer_block(x)\n",
    "  x = layers.GlobalAveragePooling1D()(x)\n",
    "  x = layers.Dropout(0.3)(x)\n",
    "  \n",
    "  x = layers.Dense(128, activation=\"relu\")(x)\n",
    "  x = layers.Dropout(0.3)(x)\n",
    "  \n",
    "  # x = layers.Dense(64, activation=\"relu\")(x)\n",
    "  # x = layers.Dropout(0.5)(x)\n",
    "\n",
    "  outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "train_df = pd.read_csv('train_enc.tsv', sep='\\t', header = None, names = ['label', 'text'], encoding='utf-8')\n",
    "test_df = pd.read_csv('dev_enc.tsv', sep='\\t', header = None, names = ['label', 'text'], encoding='utf-8')\n",
    "\n",
    "x_train, y_train, fdist = preprocess_train_or_test(train_df, top_words=20000, maxlen=200)\n",
    "x_val, y_val = preprocess_train_or_test(test_df, top_words=20000, maxlen=200, train = False, fdist = fdist)\n",
    "\n",
    "# Define a Model Checkpoint for saving best model (based on max val_accuracy)\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./model_checkpoints/transformer_model_4',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "# Initialize and Fit Model\n",
    "model = get_model()\n",
    "print(model.summary())\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=64, epochs=20, validation_data=(x_val, y_val), callbacks = [model_checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Model Evaluation\n",
    "scores = model.evaluate(x_val, y_val, verbose=0)\n",
    "print(\"Accuracy:{}\".format(scores[1]*100))\n",
    "\n",
    "# Save the trained model\n",
    "model.save('transformer_model_4')\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = tf.keras.models.load_model('transformer_model_4')\n",
    "\n",
    "# Load the best weights from the 'best' checkpoint\n",
    "loaded_model.load_weights('./model_checkpoints/transformer_model_4')\n",
    "\n",
    "# Evaluate again with best model ver.\n",
    "loaded_model.evaluate(x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Unlabeled set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.read_csv('test_enc_unlabeled.tsv', sep='\\t', header = None, names = ['text'], encoding='utf-8')\n",
    "x_pred = preprocess_predict_data(prediction_df, top_words = 20000, maxlen = 200, fdist = fdist)\n",
    "y_pred = loaded_model.predict(x_pred)\n",
    "results = np.argmax(y_pred, axis = 1)\n",
    "# Eventually, results need to be a list of 2028 0 or 1's\n",
    "results = list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Prediction Result File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to submit a prediction result file. It should have 2028 lines, every line should be either 0 or 1, which is your model's prediction on the respective test set instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose you had your model's predictions on the 2028 test cases read from test_enc_unlabeled.tsv, and \n",
    "#those results are in the list called 'results'\n",
    "assert (len(results) == 2028)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the results are not float numbers, but intergers 0 and 1\n",
    "results = [int(x) for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your prediction results to 'upload_predictions.txt' and upload that later\n",
    "with open('upload_predictions.txt', 'w', encoding = 'utf-8') as fp:\n",
    "    for x in results:\n",
    "        fp.write(str(x) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.loadtxt('upload_predictions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([1006, 1022], dtype=int64))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(result, return_counts = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
